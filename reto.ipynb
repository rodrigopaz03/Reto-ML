{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08496da0",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abcf6cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases -> idx: {'aca_bd': 0, 'aca_md': 1, 'aca_pd': 2, 'nor': 3, 'scc_bd': 4, 'scc_md': 5, 'scc_pd': 6}\n",
      "Train: 242 | Val: 52 | Test: 52\n",
      "Clases: ['aca_bd', 'aca_md', 'aca_pd', 'nor', 'scc_bd', 'scc_md', 'scc_pd']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path                \n",
    "import numpy as np                       \n",
    "from sklearn.model_selection import train_test_split  \n",
    "import torch\n",
    "from time import time\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms         \n",
    "\n",
    "DATA_DIR = Path(\"data/fold1\")            \n",
    "# transformaciones para entrenamiento\n",
    "IMG_SIZE = 224                           # tamaño al que se llevan las imágenes\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),    # redimensionar\n",
    "    transforms.ToTensor(),                      # pasar a tensor\n",
    "    transforms.Normalize(                       # normalizar colores (como ImageNet)\n",
    "        mean=[0.485,0.456,0.406],             \n",
    "        std =[0.229,0.224,0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "# transformaciones para validación y test (sin aleatoriedad)\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# cargar dataset y asignar clases a índices\n",
    "full_train = datasets.ImageFolder(root=str(DATA_DIR), transform=train_tfms)\n",
    "class_to_idx = full_train.class_to_idx            \n",
    "print(\"Clases -> idx:\", class_to_idx)            \n",
    "\n",
    "# dividir en 70% train, 15% val, 15% test \n",
    "N = len(full_train)                             \n",
    "targets = np.array([y for _, y in full_train.samples])  \n",
    "\n",
    "idx_all = np.arange(N)                          \n",
    "train_idx, tmp_idx = train_test_split(\n",
    "    idx_all, test_size=0.30, random_state=42, stratify=targets\n",
    ")\n",
    "val_idx, test_idx = train_test_split(\n",
    "    tmp_idx, test_size=0.50, random_state=42, stratify=targets[tmp_idx]\n",
    ")\n",
    "\n",
    "# datasets finales \n",
    "train_ds = Subset(full_train, train_idx)          \n",
    "eval_base = datasets.ImageFolder(root=str(DATA_DIR), transform=eval_tfms)\n",
    "val_ds   = Subset(eval_base,  val_idx)\n",
    "test_ds  = Subset(eval_base,  test_idx)\n",
    "\n",
    "# dataloaders \n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"Train: {len(train_ds)} | Val: {len(val_ds)} | Test: {len(test_ds)}\")\n",
    "print(\"Clases:\", list(class_to_idx.keys()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf5a849",
   "metadata": {},
   "source": [
    "## Entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4de3f580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\usuario\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] train_loss=0.1411 acc=0.950 | val_loss=1.1696 acc=0.673\n",
      "[02] train_loss=0.0479 acc=0.992 | val_loss=1.1858 acc=0.692\n",
      "[03] train_loss=0.0511 acc=0.992 | val_loss=1.1694 acc=0.673\n",
      "[04] train_loss=0.0393 acc=0.996 | val_loss=1.1829 acc=0.673\n",
      "[05] train_loss=0.0256 acc=0.996 | val_loss=1.1426 acc=0.712\n",
      "[06] train_loss=0.0344 acc=0.992 | val_loss=1.1582 acc=0.712\n",
      "[07] train_loss=0.0180 acc=1.000 | val_loss=1.3628 acc=0.712\n",
      "[08] train_loss=0.0273 acc=0.996 | val_loss=1.2640 acc=0.712\n",
      "Early stopping.\n",
      "Tiempo total: 294.0s\n"
     ]
    }
   ],
   "source": [
    "def fit(model, train_loader, val_loader, optimizer, scheduler=None, epochs=10, patience=3, device=\"cpu\"):\n",
    "    \n",
    "    best_val = np.inf              \n",
    "    best_state = None             \n",
    "    no_improve = 0                \n",
    "    history = []                   \n",
    "\n",
    "    t0 = time()                    # medir tiempo de entrenamiento\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # entrenar una epoca \n",
    "        tr_loss, tr_acc = run_epoch(model, train_loader, optimizer, device)\n",
    "        # evaluar en validacion\n",
    "        val_loss, val_acc = run_epoch(model, val_loader, optimizer=None, device=device)\n",
    "\n",
    "        # guardar resultados en historial\n",
    "        history.append((tr_loss, tr_acc, val_loss, val_acc))\n",
    "        print(f\"[{epoch:02d}] train_loss={tr_loss:.4f} acc={tr_acc:.3f} | \"\n",
    "              f\"val_loss={val_loss:.4f} acc={val_acc:.3f}\")\n",
    "\n",
    "        # guardar mejor modelo\n",
    "        if val_loss < best_val - 1e-4:            \n",
    "            best_val = val_loss\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:                                   \n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:           \n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    print(f\"Tiempo total: {time()-t0:.1f}s\")\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)       \n",
    "    return history\n",
    "\n",
    "# scheduler = baja el learning rate si la pérdida de validación no mejora\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "\n",
    "history = fit(model, train_loader, val_loader, optimizer, scheduler, epochs=10, patience=3, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b7be9f",
   "metadata": {},
   "source": [
    "## Evaluar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08072940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8269230769230769\n",
      "\n",
      "Reporte:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      aca_bd       1.00      0.75      0.86         8\n",
      "      aca_md       1.00      0.88      0.93         8\n",
      "      aca_pd       1.00      0.50      0.67         6\n",
      "         nor       0.80      1.00      0.89        12\n",
      "      scc_bd       0.86      1.00      0.92         6\n",
      "      scc_md       0.40      0.50      0.44         4\n",
      "      scc_pd       0.78      0.88      0.82         8\n",
      "\n",
      "    accuracy                           0.83        52\n",
      "   macro avg       0.83      0.79      0.79        52\n",
      "weighted avg       0.86      0.83      0.82        52\n",
      "\n",
      "\n",
      "Matriz de confusión:\n",
      " [[ 6  0  0  2  0  0  0]\n",
      " [ 0  7  0  1  0  0  0]\n",
      " [ 0  0  3  0  0  3  0]\n",
      " [ 0  0  0 12  0  0  0]\n",
      " [ 0  0  0  0  6  0  0]\n",
      " [ 0  0  0  0  0  2  2]\n",
      " [ 0  0  0  0  1  0  7]]\n"
     ]
    }
   ],
   "source": [
    "def evaluate_on_loader(model, loader, class_names, device=\"cpu\"):\n",
    "    model.eval()             \n",
    "    all_preds, all_true = [], []\n",
    "    for xb, yb in loader: \n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)                    \n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_true.append(yb.numpy())\n",
    "    y_pred = np.concatenate(all_preds)\n",
    "    y_true = np.concatenate(all_true)\n",
    "\n",
    "    print(\"Test Accuracy:\", (y_pred == y_true).mean())  # exactitud\n",
    "    print(\"\\nReporte:\\n\", classification_report(y_true, y_pred, target_names=class_names))\n",
    "    print(\"\\nMatriz de confusión:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    return y_true, y_pred\n",
    "\n",
    "# nombres de las clases \n",
    "try:\n",
    "    class_names = train_loader.dataset.dataset.classes\n",
    "except:\n",
    "    class_names = getattr(train_loader.dataset, 'classes', [str(i) for i in range(num_classes)])\n",
    "\n",
    "# correr evaluación en test\n",
    "y_true_test, y_pred_test = evaluate_on_loader(model, test_loader, class_names, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50eb25a",
   "metadata": {},
   "source": [
    "El modelo alcanzó una exactitud general del 83 por ciento, mostrando un buen desempeño en clases como nor, scc_bd, aca_bd y aca_md, donde logró altos valores de precisión y recall. Sin embargo, se observaron dificultades en aca_pd y especialmente en scc_md, con f1-scores bajos debido a confusiones frecuentes: aca_pd fue confundido con scc_md, y scc_md con scc_pd. Esto indica que el modelo distingue bien la mayoría de los tejidos, pero aún tiene problemas para separar subtipos con características muy similares, lo que sugiere la necesidad de aplicar más técnicas de data augmentation, ajuste fino del modelo y balanceo de clases para mejorar su rendimiento en estas categorías.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
